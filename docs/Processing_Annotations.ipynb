{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing MedCATTrainer Annotations\n",
    "A short notebook to demonstrate the MedCATTrainer downloaded annotations schema. Both w/ and w/o text have the same format, except from the source text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the annotations downloaded - as described: https://github.com/CogStack/MedCATtrainer/blob/master/README.md#download-annos\n",
    "projs = json.load(open('example_data/MedCAT_Export_With_Text_2020-05-22_10_34_09.json'))['projects']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projects annotated:2\n"
     ]
    }
   ],
   "source": [
    "# Number of annotation projects downloaded\n",
    "print(f'Projects annotated:{len(projs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'id', 'cuis', 'tuis', 'documents'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select first project\n",
    "proj = projs[0]\n",
    "# project level cui / tui filters are top level dict keys\n",
    "proj.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Documents: 2\n",
      "# of Annotations: 47\n",
      "# Validated Annotations: 47\n",
      "# Correct Annotations: 32\n",
      "# Correct Annotations: 15\n",
      "# Correct Annotations: 0\n",
      "# Correct Annotations: 0\n",
      "# Correct Annotations: 0\n"
     ]
    }
   ],
   "source": [
    "# Annotations are found inside each document.\n",
    "print(f'# of Documents: {len(proj[\"documents\"])}')\n",
    "print(f'# of Annotations: {sum([len(d[\"annotations\"]) for d in proj[\"documents\"]])}')\n",
    "\n",
    "# Annotations that have been marked by a human annotator\n",
    "print(f'# Validated Annotations: {len([a for d in proj[\"documents\"] for a in d[\"annotations\"] if a[\"validated\"] == True])}')\n",
    "\n",
    "# Annotations that have been marked correct - (blue) \n",
    "print(f'# Correct Annotations: {len([a for d in proj[\"documents\"] for a in d[\"annotations\"] if a[\"correct\"] == True])}')\n",
    "\n",
    "# Annotations that have been marked incorrect  - (red)\n",
    "print(f'# Correct Annotations: {len([a for d in proj[\"documents\"] for a in d[\"annotations\"] if a[\"deleted\"] == True])}')\n",
    "\n",
    "# Annotations that have been marked terminated - (dark red)\n",
    "print(f'# Correct Annotations: {len([a for d in proj[\"documents\"] for a in d[\"annotations\"] if a[\"killed\"] == True])}')\n",
    "\n",
    "# Annotations that have been marked alternative - (turquoise)\n",
    "print(f'# Correct Annotations: {len([a for d in proj[\"documents\"] for a in d[\"annotations\"] if a[\"alternative\"] == True])}')\n",
    "\n",
    "# Annotations that have been manually created via right-click - 'Add Annotation', these will also be 'correct' == True\n",
    "print(f'# Correct Annotations: {len([a for d in proj[\"documents\"] for a in d[\"annotations\"] if a[\"manually_created\"] == True])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Annotations \n",
    "Each Meta Annotation will have the names of the task and associated values you've previously selected.\n",
    "In this case we have: 'Negation' and 'Skip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correct Annotations that are Correct and Meta Annotation Temporarilty - Present, Experiencer - Patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Negation': {'name': 'Negation',\n",
       "  'value': 'No',\n",
       "  'acc': 1.0,\n",
       "  'validated': True},\n",
       " 'Skip': {'name': 'Skip', 'value': 'Yes', 'acc': 1.0, 'validated': True}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj['documents'][1]['annotations'][2]['meta_anns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_name</th>\n",
       "      <th>anno_value</th>\n",
       "      <th>cui</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Psych Text 1</td>\n",
       "      <td>psychopathology</td>\n",
       "      <td>C0004936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Psych Text 1</td>\n",
       "      <td>constipated</td>\n",
       "      <td>C0009806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Psych Text 1</td>\n",
       "      <td>depression</td>\n",
       "      <td>C0011570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Psych Text 1</td>\n",
       "      <td>depression</td>\n",
       "      <td>C0011570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Psych Text 1</td>\n",
       "      <td>fatigued</td>\n",
       "      <td>C0015672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc_name       anno_value       cui\n",
       "0  Psych Text 1  psychopathology  C0004936\n",
       "1  Psych Text 1      constipated  C0009806\n",
       "2  Psych Text 1       depression  C0011570\n",
       "3  Psych Text 1       depression  C0011570\n",
       "4  Psych Text 1         fatigued  C0015672"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annos = []\n",
    "for doc in proj['documents']:\n",
    "    for a in doc['annotations']:\n",
    "        meta_anns = a['meta_anns']\n",
    "        if a['correct'] == True and len(meta_anns) != 0:\n",
    "            # meta_anns are a list of dictionaries, each dict is a meta annotation. Order is not neccessarily consistent\n",
    "            negation = meta_anns['Negation']\n",
    "            skip = meta_anns['Skip']\n",
    "            if negation['value'] == 'No' and skip['value'] == 'Yes':\n",
    "                # pull out the doc_name, the text span value, and the concept\n",
    "                annos.append({'doc_name': doc['name'], 'anno_value': a['value'], 'cui': a['cui']})\n",
    "# make DataFrame\n",
    "df = pd.DataFrame(annos)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing a Second (or More) set of Annotations\n",
    "Often we'll dual annotate projects and compute metrics to develop a gold standard.\n",
    "- We'll compute metrics such [Inter Annotator Agreement (IIA)](https://en.wikipedia.org/wiki/Inter-rater_reliability) and [Cohen's Kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa).\n",
    "- Metrics can be output for each concept for the concept recognition+linking tasks.\n",
    "- For tasks with only a handful of concept filters we can compute the meta annotation task agreement, but often we will not have enough annotatinos for any meaningful. Instead we can group all meta annotations together to compute scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 46102,\n",
       " 'user': 'admin',\n",
       " 'cui': 'C1656589',\n",
       " 'value': 'oppositional defiant disorder',\n",
       " 'start': 9593,\n",
       " 'end': 9622,\n",
       " 'validated': True,\n",
       " 'correct': False,\n",
       " 'deleted': True,\n",
       " 'alternative': False,\n",
       " 'killed': False,\n",
       " 'last_modified': '2020-05-22 10:33:23.830587+00:00',\n",
       " 'manually_created': False,\n",
       " 'acc': 0.392611944253081,\n",
       " 'meta_anns': {}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj['documents'][0]['annotations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anno_state(anno):\n",
    "    if anno['deleted']:\n",
    "        return 'del'\n",
    "    if anno['alternative']:\n",
    "        return 'alt'\n",
    "    if anno['killed']:\n",
    "        return 'kil'\n",
    "    if anno['manually_created']:\n",
    "        return 'man'\n",
    "    return 'cor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concept Recognition + Linking Agreement per CUI across 2 projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only take documents completed by both\n",
    "shared_docs = set([d['id'] for d in projs[0]['documents']]) & set([d['id'] for d in projs[1]['documents']])\n",
    "projs[0]['documents'] = [d for d in projs[0]['documents'] if d['id'] in shared_docs]\n",
    "projs[1]['documents'] = [d for d in projs[1]['documents'] if d['id'] in shared_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project 1 annos\n",
    "proj1_annos_cuis = {f'{d[\"id\"]}:{a[\"start\"]}': a['cui'] for d in projs[0]['documents'] for a in d['annotations']}\n",
    "proj1_annos_states = {f'{d[\"id\"]}:{a[\"start\"]}': anno_state(a) for d in projs[0]['documents'] for a in d['annotations']}\n",
    "# project 2 annos\n",
    "proj2_annos_cuis = {f'{d[\"id\"]}:{a[\"start\"]}': a['cui'] for d in projs[1]['documents'] for a in d['annotations']}\n",
    "proj2_annos_states = {f'{d[\"id\"]}:{a[\"start\"]}': anno_state(a) for d in projs[1]['documents'] for a in d['annotations']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cuis = set(proj1_annos_cuis.values()) | set(proj2_annos_cuis.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cui_ck = {}\n",
    "for cui in all_cuis:\n",
    "    cui_tuples = []\n",
    "    p1 = {k:v for k,v in proj1_annos_cuis.items() if v == cui}\n",
    "    p2 = {k:v for k,v in proj2_annos_cuis.items() if v == cui}\n",
    "    for anno_key in set(p1.keys()) | set(p2.keys()):\n",
    "        cui_tuples.append((proj1_annos_states.get(anno_key, 'na'), proj2_annos_states.get(anno_key, 'na')))\n",
    "    cui_ck[cui] = cui_tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IIA Per CUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "iia_per_cui = {cui: (len([i for i in v if i[0] == i[1]]) / len(v)) * 100 for cui, v in cui_ck.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohen's Kappa Per CUI\n",
    "Note: for cuis with only one label it can be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tom/anaconda3/envs/cattrainer/lib/python3.7/site-packages/sklearn/metrics/_classification.py:604: RuntimeWarning: invalid value encountered in true_divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n"
     ]
    }
   ],
   "source": [
    "cohens_kappa_per_cui = {k: cohen_kappa_score([i[0] for i in v], [i[1] for i in v]) for k,v in cui_ck.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Annotation\n",
    "- Group all annos together for each task and compute IIA, CK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project 1 meta annos\n",
    "proj1_meta_annos_neg = {f'{d[\"id\"]}:{a[\"start\"]}': a['meta_anns'].get('Negation', {'value': 'na'})['value'] for d in projs[0]['documents'] for a in d['annotations']}\n",
    "proj1_meta_annos_skip = {f'{d[\"id\"]}:{a[\"start\"]}': a['meta_anns'].get('Skip', {'value': 'na'})['value'] for d in projs[0]['documents'] for a in d['annotations']}\n",
    "# project 2 meta annos\n",
    "proj2_meta_annos_neg = {f'{d[\"id\"]}:{a[\"start\"]}': a['meta_anns'].get('Negation', {'value': 'na'})['value'] for d in projs[1]['documents'] for a in d['annotations']}\n",
    "proj2_meta_annos_skip = {f'{d[\"id\"]}:{a[\"start\"]}': a['meta_anns'].get('Skip', {'value': 'na'})['value'] for d in projs[1]['documents'] for a in d['annotations']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove na examples, these would be incorret or terminated exampels that have no meta anno value. \n",
    "def remove_na(meta_annos_dict):\n",
    "    return {k:v for k,v in meta_annos_dict.items() if v != 'na'}\n",
    "proj1_meta_annos_neg = remove_na(proj1_meta_annos_neg)\n",
    "proj1_meta_annos_skip = remove_na(proj1_meta_annos_skip)\n",
    "proj2_meta_annos_neg = remove_na(proj2_meta_annos_neg)\n",
    "proj2_meta_annos_skip = remove_na(proj2_meta_annos_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take meta annos from each project and combine across projects, \n",
    "# - A more strict measure: defaulting to 'na' if there is no appropriate meta anno in the 'other' project, to use this one swap '&' (intersection) with \"|\" a union.\n",
    "# - A more fair measure: removing the instance where there was no meta anno in the other project. We use this one below.\n",
    "neg_annos = []\n",
    "for anno_key in set(proj1_meta_annos_neg.keys()) & set(proj2_meta_annos_neg.keys()):\n",
    "    neg_annos.append((proj1_meta_annos_neg.get(anno_key, 'na'), proj2_meta_annos_neg.get(anno_key, 'na')))\n",
    "\n",
    "skip_annos = []\n",
    "for anno_key in set(proj1_meta_annos_skip.keys()) & set(proj2_meta_annos_skip.keys()):\n",
    "    skip_annos.append(((proj1_meta_annos_skip.get(anno_key, 'na')), proj2_meta_annos_skip.get(anno_key, 'na')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iia neg: 100.0\n",
      "iia skip: 100.0\n"
     ]
    }
   ],
   "source": [
    "iia_neg = (len([a for a in neg_annos if a[0] == a[1]]) / len(neg_annos)) * 100\n",
    "print('iia neg:', iia_neg)\n",
    "iia_skip = (len([a for a in skip_annos if a[0] == a[1]]) / len(skip_annos)) * 100\n",
    "print('iia skip:', iia_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohen's kappa neg: nan\n",
      "cohen's kappa skip: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tom/anaconda3/envs/cattrainer/lib/python3.7/site-packages/sklearn/metrics/_classification.py:604: RuntimeWarning: invalid value encountered in true_divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n"
     ]
    }
   ],
   "source": [
    "ck_neg = cohen_kappa_score([v[0] for v in neg_annos], [v[1] for v in neg_annos])\n",
    "print(\"cohen's kappa neg:\", ck_neg)\n",
    "ck_skip = cohen_kappa_score([v[0] for v in skip_annos], [v[1] for v in skip_annos])\n",
    "print(\"cohen's kappa skip:\", ck_skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 'nan's here as there are no other values exist in the intersection of values so cohen's kappa is undefined. We can report 100% IIA though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cattrainer]",
   "language": "python",
   "name": "conda-env-cattrainer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
