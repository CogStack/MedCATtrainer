{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedCATtrainer Annotations\n",
    "A notebook to demonstrate:\n",
    "- How to download (aka export) annotations from the trainer. These downloads can also be used as export / transfer / backup option for MCTrainer.\n",
    "- The MedCATTrainer downloaded annotations schema. Both w/ and w/o text have the same format, except from the source text. \n",
    "- How to re-upload exported annotatinos into 'new' projects, this could be for recovery, or importing to a new Trainer deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Annotations\n",
    "This covers API driven downloading, and is also accessible from \\<hostname\\>:\\<port\\>/admin/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'http://localhost:8001' # Should be set to your running deployment, IP / PORT if not running on localhost:8001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Authorization': 'Token cc3e60dd2cc4231f7f74d1f30d35ce31d3154f7c'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API access is via a username / password. Upon login the API auth endpoint provides an auth token that must be used for all following requests.\n",
    "payload = {\"username\": \"admin\", \"password\": \"admin\"}\n",
    "headers = {\n",
    "    'Authorization': f'Token {json.loads(requests.post(\"http://localhost:8001/api/api-token-auth/\", json=payload).text)[\"token\"]}',\n",
    "}\n",
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 68,\n",
      "  'name': 'Top level Concept Annos (Diseases / Symptoms / Findings) (Clone)'},\n",
      " {'id': 69,\n",
      "  'name': 'Chevron test - UMLS (Diseases / Symptoms / Findings) (Clone)'},\n",
      " {'id': 70,\n",
      "  'name': 'Example Annotation Project - UMLS (Diseases / Symptoms / Findings) '\n",
      "          '(Clone)'},\n",
      " {'id': 71, 'name': 'Example Annotation Project - SNOMED CT All (Clone)'}]\n"
     ]
    }
   ],
   "source": [
    "# get project IDs to download\n",
    "resp = json.loads(requests.get(f'{URL}/api/project-annotate-entities/', headers=headers).text)['results']\n",
    "pprint([{'id': r['id'], 'name': r['name']} for r in resp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projects to download\n",
    "projects_to_download = ','.join(str(r['id']) for r in resp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further parameters available here are:\n",
    "# - with_text: Boolean: to download the annotations with the source document text. This will automatically include the doc_name. Default: False\n",
    "# - with_doc_name: Boolean: if with_text is False, but you still want to include doc names. Default: False\n",
    "\n",
    "resp = json.loads(requests.get(f'{URL}/api/download-annos/?project_ids={projects_to_download}&with_text=True', headers=headers).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump the output to a file.\n",
    "json.dump(resp, open('trainer_export.json', 'w')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing MedCATTrainer Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the annotations downloaded - as described: https://github.com/CogStack/MedCATtrainer/blob/master/README.md#download-annos\n",
    "projs = json.load(open('example_data/MedCAT_Export_With_Text_2020-05-22_10_34_09.json'))['projects']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projects annotated:2\n"
     ]
    }
   ],
   "source": [
    "# Number of annotation projects downloaded\n",
    "print(f'Projects annotated:{len(projs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'id', 'cuis', 'tuis', 'documents'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select first project\n",
    "proj = projs[0]\n",
    "# project level cui / tui filters are top level dict keys\n",
    "proj.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Documents: 2\n",
      "# of Annotations: 47\n",
      "# Validated Annotations: 47\n",
      "# Correct Annotations: 32\n",
      "# Correct Annotations: 15\n",
      "# Correct Annotations: 0\n",
      "# Correct Annotations: 0\n",
      "# Correct Annotations: 0\n"
     ]
    }
   ],
   "source": [
    "# Annotations are found inside each document.\n",
    "print(f'# of Documents: {len(proj[\"documents\"])}')\n",
    "print(f'# of Annotations: {sum([len(d[\"annotations\"]) for d in proj[\"documents\"]])}')\n",
    "\n",
    "# Annotations that have been marked by a human annotator\n",
    "print(f'# Validated Annotations: {len([a for d in proj[\"documents\"] for a in d[\"annotations\"] if a[\"validated\"] == True])}')\n",
    "\n",
    "# Annotations that have been marked correct - (blue) \n",
    "print(f'# Correct Annotations: {len([a for d in proj[\"documents\"] for a in d[\"annotations\"] if a[\"correct\"] == True])}')\n",
    "\n",
    "# Annotations that have been marked incorrect  - (red)\n",
    "print(f'# Correct Annotations: {len([a for d in proj[\"documents\"] for a in d[\"annotations\"] if a[\"deleted\"] == True])}')\n",
    "\n",
    "# Annotations that have been marked terminated - (dark red)\n",
    "print(f'# Correct Annotations: {len([a for d in proj[\"documents\"] for a in d[\"annotations\"] if a[\"killed\"] == True])}')\n",
    "\n",
    "# Annotations that have been marked alternative - (turquoise)\n",
    "print(f'# Correct Annotations: {len([a for d in proj[\"documents\"] for a in d[\"annotations\"] if a[\"alternative\"] == True])}')\n",
    "\n",
    "# Annotations that have been manually created via right-click - 'Add Annotation', these will also be 'correct' == True\n",
    "print(f'# Correct Annotations: {len([a for d in proj[\"documents\"] for a in d[\"annotations\"] if a[\"manually_created\"] == True])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Annotations \n",
    "Each Meta Annotation will have the names of the task and associated values you've previously selected.\n",
    "In this case we have: 'Negation' and 'Skip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correct Annotations that are Correct and Meta Annotation Temporarilty - Present, Experiencer - Patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Negation': {'name': 'Negation',\n",
       "  'value': 'No',\n",
       "  'acc': 1.0,\n",
       "  'validated': True},\n",
       " 'Skip': {'name': 'Skip', 'value': 'Yes', 'acc': 1.0, 'validated': True}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj['documents'][1]['annotations'][2]['meta_anns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_name</th>\n",
       "      <th>anno_value</th>\n",
       "      <th>cui</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Psych Text 1</td>\n",
       "      <td>psychopathology</td>\n",
       "      <td>C0004936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Psych Text 1</td>\n",
       "      <td>constipated</td>\n",
       "      <td>C0009806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Psych Text 1</td>\n",
       "      <td>depression</td>\n",
       "      <td>C0011570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Psych Text 1</td>\n",
       "      <td>depression</td>\n",
       "      <td>C0011570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Psych Text 1</td>\n",
       "      <td>fatigued</td>\n",
       "      <td>C0015672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc_name       anno_value       cui\n",
       "0  Psych Text 1  psychopathology  C0004936\n",
       "1  Psych Text 1      constipated  C0009806\n",
       "2  Psych Text 1       depression  C0011570\n",
       "3  Psych Text 1       depression  C0011570\n",
       "4  Psych Text 1         fatigued  C0015672"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annos = []\n",
    "for doc in proj['documents']:\n",
    "    for a in doc['annotations']:\n",
    "        meta_anns = a['meta_anns']\n",
    "        if a['correct'] == True and len(meta_anns) != 0:\n",
    "            # meta_anns are a list of dictionaries, each dict is a meta annotation. Order is not neccessarily consistent\n",
    "            negation = meta_anns['Negation']\n",
    "            skip = meta_anns['Skip']\n",
    "            if negation['value'] == 'No' and skip['value'] == 'Yes':\n",
    "                # pull out the doc_name, the text span value, and the concept\n",
    "                annos.append({'doc_name': doc['name'], 'anno_value': a['value'], 'cui': a['cui']})\n",
    "# make DataFrame\n",
    "df = pd.DataFrame(annos)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing a Second (or More) set of Annotations\n",
    "Often we'll dual annotate projects and compute metrics to develop a gold standard.\n",
    "- We'll compute metrics such [Inter Annotator Agreement (IIA)](https://en.wikipedia.org/wiki/Inter-rater_reliability) and [Cohen's Kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa).\n",
    "- Metrics can be output for each concept for the concept recognition+linking tasks.\n",
    "- For tasks with only a handful of concept filters we can compute the meta annotation task agreement, but often we will not have enough annotatinos for any meaningful. Instead we can group all meta annotations together to compute scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 46102,\n",
       " 'user': 'admin',\n",
       " 'cui': 'C1656589',\n",
       " 'value': 'oppositional defiant disorder',\n",
       " 'start': 9593,\n",
       " 'end': 9622,\n",
       " 'validated': True,\n",
       " 'correct': False,\n",
       " 'deleted': True,\n",
       " 'alternative': False,\n",
       " 'killed': False,\n",
       " 'last_modified': '2020-05-22 10:33:23.830587+00:00',\n",
       " 'manually_created': False,\n",
       " 'acc': 0.392611944253081,\n",
       " 'meta_anns': {}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj['documents'][0]['annotations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anno_state(anno):\n",
    "    if anno['deleted']:\n",
    "        return 'del'\n",
    "    if anno['alternative']:\n",
    "        return 'alt'\n",
    "    if anno['killed']:\n",
    "        return 'kil'\n",
    "    if anno['manually_created']:\n",
    "        return 'man'\n",
    "    return 'cor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concept Recognition + Linking Agreement per CUI across 2 projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only take documents completed by both\n",
    "shared_docs = set([d['id'] for d in projs[0]['documents']]) & set([d['id'] for d in projs[1]['documents']])\n",
    "projs[0]['documents'] = [d for d in projs[0]['documents'] if d['id'] in shared_docs]\n",
    "projs[1]['documents'] = [d for d in projs[1]['documents'] if d['id'] in shared_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project 1 annos\n",
    "proj1_annos_cuis = {f'{d[\"id\"]}:{a[\"start\"]}': a['cui'] for d in projs[0]['documents'] for a in d['annotations']}\n",
    "proj1_annos_states = {f'{d[\"id\"]}:{a[\"start\"]}': anno_state(a) for d in projs[0]['documents'] for a in d['annotations']}\n",
    "# project 2 annos\n",
    "proj2_annos_cuis = {f'{d[\"id\"]}:{a[\"start\"]}': a['cui'] for d in projs[1]['documents'] for a in d['annotations']}\n",
    "proj2_annos_states = {f'{d[\"id\"]}:{a[\"start\"]}': anno_state(a) for d in projs[1]['documents'] for a in d['annotations']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cuis = set(proj1_annos_cuis.values()) | set(proj2_annos_cuis.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cui_ck = {}\n",
    "for cui in all_cuis:\n",
    "    cui_tuples = []\n",
    "    p1 = {k:v for k,v in proj1_annos_cuis.items() if v == cui}\n",
    "    p2 = {k:v for k,v in proj2_annos_cuis.items() if v == cui}\n",
    "    for anno_key in set(p1.keys()) | set(p2.keys()):\n",
    "        cui_tuples.append((proj1_annos_states.get(anno_key, 'na'), proj2_annos_states.get(anno_key, 'na')))\n",
    "    cui_ck[cui] = cui_tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IIA Per CUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "iia_per_cui = {cui: (len([i for i in v if i[0] == i[1]]) / len(v)) * 100 for cui, v in cui_ck.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohen's Kappa Per CUI\n",
    "Note: for cuis with only one label it can be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/cattrainer/lib/python3.8/site-packages/sklearn/metrics/_classification.py:620: RuntimeWarning: invalid value encountered in true_divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n"
     ]
    }
   ],
   "source": [
    "cohens_kappa_per_cui = {k: cohen_kappa_score([i[0] for i in v], [i[1] for i in v]) for k,v in cui_ck.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Annotation\n",
    "- Group all annos together for each task and compute IIA, CK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project 1 meta annos\n",
    "proj1_meta_annos_neg = {f'{d[\"id\"]}:{a[\"start\"]}': a['meta_anns'].get('Negation', {'value': 'na'})['value'] for d in projs[0]['documents'] for a in d['annotations']}\n",
    "proj1_meta_annos_skip = {f'{d[\"id\"]}:{a[\"start\"]}': a['meta_anns'].get('Skip', {'value': 'na'})['value'] for d in projs[0]['documents'] for a in d['annotations']}\n",
    "# project 2 meta annos\n",
    "proj2_meta_annos_neg = {f'{d[\"id\"]}:{a[\"start\"]}': a['meta_anns'].get('Negation', {'value': 'na'})['value'] for d in projs[1]['documents'] for a in d['annotations']}\n",
    "proj2_meta_annos_skip = {f'{d[\"id\"]}:{a[\"start\"]}': a['meta_anns'].get('Skip', {'value': 'na'})['value'] for d in projs[1]['documents'] for a in d['annotations']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove na examples, these would be incorret or terminated exampels that have no meta anno value. \n",
    "def remove_na(meta_annos_dict):\n",
    "    return {k:v for k,v in meta_annos_dict.items() if v != 'na'}\n",
    "proj1_meta_annos_neg = remove_na(proj1_meta_annos_neg)\n",
    "proj1_meta_annos_skip = remove_na(proj1_meta_annos_skip)\n",
    "proj2_meta_annos_neg = remove_na(proj2_meta_annos_neg)\n",
    "proj2_meta_annos_skip = remove_na(proj2_meta_annos_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take meta annos from each project and combine across projects, \n",
    "# - A more strict measure: defaulting to 'na' if there is no appropriate meta anno in the 'other' project, to use this one swap '&' (intersection) with \"|\" a union.\n",
    "# - A more fair measure: removing the instance where there was no meta anno in the other project. We use this one below.\n",
    "neg_annos = []\n",
    "for anno_key in set(proj1_meta_annos_neg.keys()) & set(proj2_meta_annos_neg.keys()):\n",
    "    neg_annos.append((proj1_meta_annos_neg.get(anno_key, 'na'), proj2_meta_annos_neg.get(anno_key, 'na')))\n",
    "\n",
    "skip_annos = []\n",
    "for anno_key in set(proj1_meta_annos_skip.keys()) & set(proj2_meta_annos_skip.keys()):\n",
    "    skip_annos.append(((proj1_meta_annos_skip.get(anno_key, 'na')), proj2_meta_annos_skip.get(anno_key, 'na')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iia neg: 100.0\n",
      "iia skip: 100.0\n"
     ]
    }
   ],
   "source": [
    "iia_neg = (len([a for a in neg_annos if a[0] == a[1]]) / len(neg_annos)) * 100\n",
    "print('iia neg:', iia_neg)\n",
    "iia_skip = (len([a for a in skip_annos if a[0] == a[1]]) / len(skip_annos)) * 100\n",
    "print('iia skip:', iia_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohen's kappa neg: nan\n",
      "cohen's kappa skip: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tom/anaconda3/envs/cattrainer/lib/python3.7/site-packages/sklearn/metrics/_classification.py:604: RuntimeWarning: invalid value encountered in true_divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n"
     ]
    }
   ],
   "source": [
    "ck_neg = cohen_kappa_score([v[0] for v in neg_annos], [v[1] for v in neg_annos])\n",
    "print(\"cohen's kappa neg:\", ck_neg)\n",
    "ck_skip = cohen_kappa_score([v[0] for v in skip_annos], [v[1] for v in skip_annos])\n",
    "print(\"cohen's kappa skip:\", ck_skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 'nan's here as there are no other values exist in the intersection of values so cohen's kappa is undefined. We can report 100% IIA though!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Annotations\n",
    "This is useful if annotations have been exported, and need to be systematically modified or if a new instance of Trainer is required to be used for further annotations\n",
    "\n",
    "This will use the previously exported trainer download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_data = json.load(open('trainer_export.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out project data that is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_data['projects'] = [p for p in project_data['projects'] if len(p['documents']) > 0 ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 68,\n",
      "  'name': 'Top level Concept Annos (Diseases / Symptoms / Findings) (Clone)'},\n",
      " {'id': 69,\n",
      "  'name': 'Chevron test - UMLS (Diseases / Symptoms / Findings) (Clone)'},\n",
      " {'id': 70,\n",
      "  'name': 'Example Annotation Project - UMLS (Diseases / Symptoms / Findings) '\n",
      "          '(Clone)'},\n",
      " {'id': 71, 'name': 'Example Annotation Project - SNOMED CT All (Clone)'}]\n"
     ]
    }
   ],
   "source": [
    "# current projects projects\n",
    "resp = json.loads(requests.get(f'{URL}/api/project-annotate-entities/', headers=headers).text)['results']\n",
    "pprint([{'id': r['id'], 'name': r['name']} for r in resp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload previoulsy exported projects\n",
    "resp = requests.post(f'{URL}/api/upload-deployment/', json=project_data).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"successfully uploaded\"\n"
     ]
    }
   ],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 68,\n",
      "  'name': 'Top level Concept Annos (Diseases / Symptoms / Findings) (Clone)'},\n",
      " {'id': 69,\n",
      "  'name': 'Chevron test - UMLS (Diseases / Symptoms / Findings) (Clone)'},\n",
      " {'id': 70,\n",
      "  'name': 'Example Annotation Project - UMLS (Diseases / Symptoms / Findings) '\n",
      "          '(Clone)'},\n",
      " {'id': 71, 'name': 'Example Annotation Project - SNOMED CT All (Clone)'},\n",
      " {'id': 81,\n",
      "  'name': 'Chevron test - UMLS (Diseases / Symptoms / Findings) (Clone) '\n",
      "          'IMPORTED'},\n",
      " {'id': 82,\n",
      "  'name': 'Example Annotation Project - UMLS (Diseases / Symptoms / Findings) '\n",
      "          '(Clone) IMPORTED'}]\n"
     ]
    }
   ],
   "source": [
    "# to show the newly uploaded projects\n",
    "resp = json.loads(requests.get(f'{URL}/api/project-annotate-entities/', headers=headers).text)['results']\n",
    "pprint([{'id': r['id'], 'name': r['name']} for r in resp])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cattrainer]",
   "language": "python",
   "name": "conda-env-cattrainer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
